{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa4O-JzC3KVy"
      },
      "source": [
        "Debugging Google drive connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtaAh8BA2TeR",
        "outputId": "a70cafa9-6d18-4b7f-90ca-121c482cf482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "'Copy of openwakeword_trainer_fixed.ipynb'\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ðŸŽ¤ OpenWakeWord Model Trainer\n",
        "\n",
        "Train custom wake word models for [openWakeWord](https://github.com/dscripka/openWakeWord).\n",
        "\n",
        "## Features\n",
        "- âœ… Train **multiple wake words** at once\n",
        "- âœ… **Better parameter controls** with clear explanations\n",
        "- âœ… **Quick test mode** for fast iteration\n",
        "- âœ… **Progress logging** with clear status indicators\n",
        "- âœ… **Google Drive integration** for reliable model saving\n",
        "- âœ… Produces **ONNX models** (works with Home Assistant, Python, etc.)\n",
        "\n",
        "## How to Use\n",
        "\n",
        "**Note:** All steps must be executed in order!\n",
        "\n",
        "1. **Step 1**: Test pronunciation - make sure your wake word sounds right\n",
        "2. **Step 2**: Configure training parameters\n",
        "3. **Step 3**: Download data (~15-20 min)\n",
        "4. **Step 4**: Train model (~30-90 min depending on settings) - includes Google Drive option!\n",
        "5. **Step 5**: Download your `.onnx` model file (backup if not using Google Drive)\n",
        "\n",
        "**Tip:** Use GPU runtime for faster training: Runtime â†’ Change runtime type â†’ T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "step1_preview",
        "outputId": "9c420c77-945b-4c04-9d27-f36344433b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”§ First run - setting up TTS engine (~1-2 minutes)...\n",
            "Cloning into 'piper-sample-generator'...\n",
            "remote: Enumerating objects: 161, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 161 (delta 64), reused 62 (delta 50), pack-reused 69 (from 1)\u001b[K\n",
            "Receiving objects: 100% (161/161), 1.04 MiB | 14.21 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n",
            "Note: switching to '213d4d5'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at 213d4d5 revert removing webrtcvad\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m137.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m144.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.1.0 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.1.0 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ… TTS setup complete!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/webrtcvad.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â–¶ï¸ Listen to your wake word:\n",
            "ðŸŽ¤ Generating audio for: 'how_you_do_this!?'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
              "                    <source src=\"data:audio/x-wav;base64,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\" type=\"audio/x-wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ’¡ If it doesn't sound right, change the spelling above and run again!\n",
            "   Once satisfied, proceed to Step 2.\n"
          ]
        }
      ],
      "source": [
        "# @title ## ðŸŽ§ Step 1: Test Wake Word Pronunciation { display-mode: \"form\" }\n",
        "# @markdown **Test how your wake word will sound before training!**\n",
        "# @markdown\n",
        "# @markdown First run takes ~1-2 minutes to setup. Subsequent runs are fast.\n",
        "# @markdown\n",
        "# @markdown ### Pronunciation Tips\n",
        "# @markdown - Use underscores for syllable breaks: `computer` â†’ `khum_puter`\n",
        "# @markdown - Spell phonetically: `jarvis` â†’ `jar_viss`\n",
        "# @markdown - Multi-word: `hey jarvis` â†’ `hey_jar_viss`\n",
        "# @markdown - Spell out numbers: `2` â†’ `two`\n",
        "# @markdown - Avoid punctuation except `?` and `!`\n",
        "\n",
        "target_word = 'how_you_do_this!?' # @param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# Setup TTS on first run\n",
        "if not os.path.exists(\"./piper-sample-generator\"):\n",
        "    print(\"ðŸ”§ First run - setting up TTS engine (~1-2 minutes)...\")\n",
        "    !git clone https://github.com/rhasspy/piper-sample-generator\n",
        "    !wget -q -O piper-sample-generator/models/en_US-libritts_r-medium.pt 'https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/en_US-libritts_r-medium.pt'\n",
        "    !cd piper-sample-generator && git checkout 213d4d5\n",
        "    !pip install -q piper-tts piper-phonemize-cross\n",
        "    !pip install -q webrtcvad\n",
        "    !pip install -q --force-reinstall 'torch==2.4.0' 'torchaudio==2.4.0' torchvision\n",
        "    print(\"âœ… TTS setup complete!\\n\")\n",
        "\n",
        "if \"piper-sample-generator/\" not in sys.path:\n",
        "    sys.path.append(\"piper-sample-generator/\")\n",
        "\n",
        "# Check for torch/torchaudio compatibility\n",
        "try:\n",
        "    import torchaudio\n",
        "    torchaudio.load  # Test that it works\n",
        "except (OSError, ImportError) as e:\n",
        "    if \"undefined symbol\" in str(e) or \"libtorchaudio\" in str(e):\n",
        "        print(\"âš ï¸ Torch/torchaudio version mismatch detected!\")\n",
        "        print(\"   This happens if Step 3 was run before Step 1.\")\n",
        "        print(\"   Please: Runtime â†’ Restart session, then run Step 1 first.\")\n",
        "        raise RuntimeError(\"Please restart runtime and run Step 1 before Step 3\")\n",
        "\n",
        "# ============================================================\n",
        "# FIX: Patch torch.load ONLY ONCE to avoid recursion error\n",
        "# ============================================================\n",
        "import torch\n",
        "\n",
        "# Check if we've already patched torch.load (prevents RecursionError on re-run)\n",
        "if not getattr(torch.load, '_oww_patched', False):\n",
        "    _original_torch_load = torch.load\n",
        "\n",
        "    def _patched_torch_load(*args, **kwargs):\n",
        "        kwargs.setdefault('weights_only', False)\n",
        "        return _original_torch_load(*args, **kwargs)\n",
        "\n",
        "    # Mark it as patched so we don't do it again\n",
        "    _patched_torch_load._oww_patched = True\n",
        "    torch.load = _patched_torch_load\n",
        "\n",
        "from generate_samples import generate_samples\n",
        "\n",
        "def preview_wake_word(text):\n",
        "    \"\"\"Generate and play a sample of the wake word.\"\"\"\n",
        "    print(f\"ðŸŽ¤ Generating audio for: '{text}'\")\n",
        "    generate_samples(\n",
        "        text=text,\n",
        "        max_samples=1,\n",
        "        length_scales=[1.1],\n",
        "        noise_scales=[0.7],\n",
        "        noise_scale_ws=[0.7],\n",
        "        output_dir='./',\n",
        "        batch_size=1,\n",
        "        auto_reduce_batch_size=True,\n",
        "        file_names=[\"test_generation.wav\"]\n",
        "    )\n",
        "    return Audio(\"test_generation.wav\", autoplay=True)\n",
        "\n",
        "print(\"\\nâ–¶ï¸ Listen to your wake word:\")\n",
        "display(preview_wake_word(target_word))\n",
        "print(\"\\nðŸ’¡ If it doesn't sound right, change the spelling above and run again!\")\n",
        "print(\"   Once satisfied, proceed to Step 2.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "step2_config",
        "outputId": "85314d1b-8ccf-4706-fe94-63a131d9521a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸ“‹ TRAINING CONFIGURATION\n",
            "==================================================\n",
            "\n",
            "ðŸŽ¯ Wake words to train: ['how_do_you_wanna_do_this!?', 'how_you_wanna_do_this!?', 'how_you_do_this!?']\n",
            "\n",
            "ðŸ“Š Training parameters:\n",
            "   â€¢ Examples per word: 30,000\n",
            "   â€¢ Training steps: 30,000\n",
            "   â€¢ False activation penalty: 2000\n",
            "\n",
            "ðŸ“ˆ Evaluation targets:\n",
            "   â€¢ Target FP/hour: 0.7\n",
            "   â€¢ Target recall: 70%\n",
            "\n",
            "ðŸ§  Model architecture:\n",
            "   â€¢ Layer size: 96 neurons\n",
            "\n",
            "âœ… Configuration saved. Proceed to Step 3.\n"
          ]
        }
      ],
      "source": [
        "# @title ## âš™ï¸ Step 2: Training Configuration { display-mode: \"form\" }\n",
        "# @markdown ### Wake Words\n",
        "# @markdown Enter one or more wake words, separated by commas.\n",
        "# @markdown Use the exact spelling that sounded best in Step 1!\n",
        "# @markdown\n",
        "# @markdown **Examples:** `hey_jar_viss` or `hey_jar_viss, oh_kay_computer`\n",
        "\n",
        "wake_words = \"how_do_you_wanna_do_this!?,  how_you_wanna_do_this!?, how_you_do_this!?\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### Quick Test Mode\n",
        "# @markdown Enable for faster training (~30 min) with lower quality. Good for testing!\n",
        "quick_test_mode = False # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### Training Parameters\n",
        "# @markdown These are ignored if Quick Test Mode is enabled.\n",
        "# @markdown\n",
        "# @markdown | Parameter | Low | Default | High | Effect |\n",
        "# @markdown |-----------|-----|---------|------|--------|\n",
        "# @markdown | Examples | 5,000 | 25,000 | 50,000 | More = better quality, longer training |\n",
        "# @markdown | Steps | 5,000 | 25,000 | 50,000 | More = better convergence, longer training |\n",
        "# @markdown | False Activation Penalty | 500 | 1,500 | 3,000 | Higher = fewer false triggers, may miss quiet speech |\n",
        "\n",
        "_number_of_examples = 30000 # @param {type:\"slider\", min:1000, max:50000, step:1000}\n",
        "_number_of_training_steps = 30000 # @param {type:\"slider\", min:1000, max:50000, step:1000}\n",
        "_false_activation_penalty = 2000 # @param {type:\"slider\", min:100, max:5000, step:100}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### Advanced Options\n",
        "# @markdown\n",
        "# @markdown **target_false_positives_per_hour** - How often model incorrectly triggers\n",
        "# @markdown - `0.1` = ~1 false trigger every 10 hours (very strict)\n",
        "# @markdown - `0.5` = ~1 false trigger every 2 hours (stricter)\n",
        "# @markdown - `1.0` = ~1 false trigger per hour (permissive)\n",
        "\n",
        "target_false_positives_per_hour = 0.7 # @param {type:\"number\"}\n",
        "\n",
        "# @markdown **target_recall** - Percentage of real wake words detected (at evaluation threshold)\n",
        "# @markdown - `0.5` = 50% detected (conservative, fewer false positives)\n",
        "# @markdown - `0.7` = 70% detected (balanced)\n",
        "# @markdown - `0.9` = 90% detected (sensitive, more false positives)\n",
        "\n",
        "target_recall = 0.7 # @param {type:\"number\"}\n",
        "\n",
        "# @markdown **layer_size** - Neural network hidden layer size (affects model size and accuracy)\n",
        "# @markdown - `32` = ~15 KB model, fastest inference, good for simple single words\n",
        "# @markdown - `64` = ~30 KB model, fast, balanced\n",
        "# @markdown - `96` = ~50 KB model, better accuracy for multi-word phrases\n",
        "# @markdown - `128` = ~75 KB model, best accuracy, slower inference\n",
        "\n",
        "layer_size = 96 # @param [32, 64, 96, 128] {type:\"raw\"}\n",
        "\n",
        "# Hidden defaults (not exposed in UI)\n",
        "target_accuracy = 0.7  # Not configurable - doesn't significantly affect training\n",
        "\n",
        "# ============================================================\n",
        "# APPLY SETTINGS\n",
        "# ============================================================\n",
        "\n",
        "# Parse wake words\n",
        "wake_word_list = [w.strip() for w in wake_words.split(',') if w.strip()]\n",
        "\n",
        "if not wake_word_list:\n",
        "    raise ValueError(\"âŒ No wake words specified! Enter at least one wake word above.\")\n",
        "\n",
        "# Apply quick test mode\n",
        "if quick_test_mode:\n",
        "    number_of_examples = 5000\n",
        "    number_of_training_steps = 5000\n",
        "    false_activation_penalty = 500\n",
        "    print(\"âš¡ QUICK TEST MODE ENABLED\")\n",
        "    print(\"   Using reduced settings for faster training (~30 min)\")\n",
        "    print(\"   Model quality will be lower - for testing only!\")\n",
        "else:\n",
        "    number_of_examples = _number_of_examples\n",
        "    number_of_training_steps = _number_of_training_steps\n",
        "    false_activation_penalty = _false_activation_penalty\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"ðŸ“‹ TRAINING CONFIGURATION\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"\\nðŸŽ¯ Wake words to train: {wake_word_list}\")\n",
        "print(f\"\\nðŸ“Š Training parameters:\")\n",
        "print(f\"   â€¢ Examples per word: {number_of_examples:,}\")\n",
        "print(f\"   â€¢ Training steps: {number_of_training_steps:,}\")\n",
        "print(f\"   â€¢ False activation penalty: {false_activation_penalty}\")\n",
        "print(f\"\\nðŸ“ˆ Evaluation targets:\")\n",
        "print(f\"   â€¢ Target FP/hour: {target_false_positives_per_hour}\")\n",
        "print(f\"   â€¢ Target recall: {target_recall*100:.0f}%\")\n",
        "print(f\"\\nðŸ§  Model architecture:\")\n",
        "print(f\"   â€¢ Layer size: {layer_size} neurons\")\n",
        "print(f\"\\nâœ… Configuration saved. Proceed to Step 3.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "421df195e35f4850aff87e9d2555c460",
            "8463a8f7fb554b1bbb6d3ba308a84469",
            "809a9fe89abc4be3a387c5ee6819344a",
            "1b133123024b413281257d46b81c9e8e",
            "08e3acb099424fbfa3da78fc1eebf7d7",
            "6dd8c36806f844a39d6369193d865054",
            "efecf1a6b1874dab9501f64f40644e14",
            "26cb651928d44a559bd15ef48c2ac44a",
            "05cc5e6929e14caf8858ea24bb6bcea9",
            "8171c39e38fa415994d6616144c6d1ea",
            "d688eb8606414c64b62749021e4e7697",
            "f3f1a7bc816c4727a1f993a5f9e5b57f",
            "de82196453cc400fbe90163bd873336e",
            "dd3411d25355420bab2eed01f65f8d1a",
            "f75c499b1d724e019d068098cf2112ff",
            "bb325949c78c4b54abd0c925882b7574",
            "766b28b2aca64ab3a99dc06d25cea186",
            "294ccd510c8543398f508691e6954138",
            "81b5e50a85c84000a96727efdca61839",
            "cbed3a5442b94df286b3fc787f2889cd",
            "c686b096009f4b6081fb084bd7c6429a",
            "e32d89205eae47f19bfd3ddc6e19de78",
            "30081236282042dcac967cf84ce8948f",
            "6d1f83fecac24d9ab3f872e3a7385aea",
            "30ac513416ae487bb403913e665a86e5",
            "e74a0beb5d9d4420bc5b5ad9702677cb",
            "62177c5656c1439dbd804caedeed8b49",
            "59dd67ecc6c74472a0a128efecf22ea3",
            "4cc9d5f0a4374713bc09cfe82d481fef",
            "25cd4e75474d43aab56da846a77841f7",
            "5c3451b379e34d55ad0c6afd24637eb4",
            "7648886f26724a949c4c892109df96cc",
            "2256dc63f4aa4813bccb9aba4f80d569",
            "6f5d1c2e59904b5fa1f7533838a423ca",
            "cb604e5737c74ebd83d8b15783e0e5bf",
            "ff6a41d55d5741cfa7c164b254a7c8f3",
            "60595598068b483594d9b38d9e610c13",
            "4e32e4b8b99c492f975685849e5e920a",
            "6d908bf28df44b768d2f97d5f5478dae",
            "f01ff8764c46462a9f1f61aa7cf9d5c5",
            "a14dcee8015c495491d39093e6625a59",
            "63c5fd23ed504dd38f12a8bde9b5db35",
            "2d667412a92d41a0baedc878090053ef",
            "5800542945ee4d6380a8bc7808c607e0"
          ]
        },
        "id": "step3_download",
        "outputId": "822956e6-1223-4d8e-fb03-3b8c119b2415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ðŸ“¦ STEP 3: ENVIRONMENT SETUP & DATA DOWNLOAD\n",
            "============================================================\n",
            "\n",
            "ðŸ”§ Installing dependencies...\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.36.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "access 1.1.10.post3 requires scipy>=1.14.1, but you have scipy 1.13.1 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.1.0 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for openwakeword (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m519.0/519.0 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m788.2/788.2 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.8/76.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m248.5/248.5 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for acoustics (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for onnxsim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "openwakeword 0.6.0 requires speexdsp-ns<1,>=0.1.2; platform_system == \"Linux\", which is not installed.\n",
            "openwakeword 0.6.0 requires ai-edge-litert<3,>=2.0.2; platform_system == \"Linux\" or platform_system == \"Darwin\", but you have ai-edge-litert 1.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m186.6/186.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m939.7/939.7 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pronouncing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.10.0 which is incompatible.\n",
            "gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep-phonemizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "âœ… Dependencies installed\n",
            "   numpy version: 1.26.4\n",
            "   scipy version: 1.13.1\n",
            "\n",
            "ðŸ“¥ Downloading openWakeWord model files...\n",
            "   âœ… embedding_model.onnx\n",
            "   âœ… embedding_model.tflite\n",
            "   âœ… melspectrogram.onnx\n",
            "   âœ… melspectrogram.tflite\n",
            "\n",
            "ðŸ“¥ Downloading room impulse responses...\n",
            "Git LFS initialized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing RIRs:   0%|          | 0/270 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "421df195e35f4850aff87e9d2555c460"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… 270 RIR files\n",
            "\n",
            "ðŸ“¥ Downloading background audio...\n",
            "   â­ï¸ Skipping AudioSet (dataset recently restructured)\n",
            "   Using FMA + pre-computed features for background audio instead.\n",
            "   Loading FMA dataset (streaming)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3f1a7bc816c4727a1f993a5f9e5b57f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30081236282042dcac967cf84ce8948f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing FMA:   0%|          | 0/360 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f5d1c2e59904b5fa1f7533838a423ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… 360 FMA files\n",
            "\n",
            "ðŸ“¥ Downloading pre-computed features (this is the big download)...\n",
            "   â¬‡ï¸ Downloading training features (~17 GB)...\n",
            "   This may take 10-30 minutes depending on connection speed.\n",
            "openwakeword_featur 100%[===================>]  16.09G  87.3MB/s    in 3m 42s  \n",
            "   âœ… Training features downloaded\n",
            "   â¬‡ï¸ Downloading validation features (~176 MB)...\n",
            "validation_set_feat 100%[===================>] 176.27M  29.1MB/s    in 6.1s    \n",
            "   âœ… Validation features downloaded\n",
            "\n",
            "============================================================\n",
            "ðŸ“Š VERIFICATION\n",
            "============================================================\n",
            "   âœ… RIRs: 270 files (need 100+)\n",
            "   â­ï¸ AudioSet: 0 files (optional)\n",
            "   âœ… FMA: 360 files (need 50+)\n",
            "   âœ… Training features (16.1 GB)\n",
            "   âœ… Validation features (176 MB)\n",
            "\n",
            "   Total background audio: 360 files (need 50+)\n",
            "\n",
            "============================================================\n",
            "âœ… STEP 3 COMPLETE - All data downloaded!\n",
            "============================================================\n",
            "\n",
            "ðŸ‘‰ Proceed to Step 4 to train your model.\n"
          ]
        }
      ],
      "source": [
        "# @title ## ðŸ“¦ Step 3: Download Data & Setup Environment { display-mode: \"form\" }\n",
        "# @markdown This downloads all required data and installs dependencies.\n",
        "# @markdown\n",
        "# @markdown **Time:** ~15-20 minutes (mostly downloading)\n",
        "# @markdown\n",
        "# @markdown **What gets downloaded:**\n",
        "# @markdown - Pre-computed audio features (~17 GB) - for negative examples\n",
        "# @markdown - Validation features (~176 MB) - for false positive testing\n",
        "# @markdown - Room impulse responses - for reverb augmentation\n",
        "# @markdown - Background audio (music/noise) - for augmentation\n",
        "# @markdown\n",
        "# @markdown âš ï¸ **License Note:** Data has mixed licenses. Models trained here are for **non-commercial personal use only**.\n",
        "\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale=True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸ“¦ STEP 3: ENVIRONMENT SETUP & DATA DOWNLOAD\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================================\n",
        "# 3.1 INSTALL DEPENDENCIES (must happen before scipy/numpy imports!)\n",
        "# ============================================================\n",
        "print(\"\\nðŸ”§ Installing dependencies...\")\n",
        "\n",
        "# Unload any cached numpy/scipy modules to avoid version conflicts\n",
        "mods_to_remove = [k for k in sys.modules.keys() if k.startswith(('numpy', 'scipy'))]\n",
        "for mod in mods_to_remove:\n",
        "    del sys.modules[mod]\n",
        "\n",
        "# Fix numpy/scipy compatibility FIRST\n",
        "!pip install -q --force-reinstall 'numpy==1.26.4' 'scipy==1.13.1'\n",
        "\n",
        "!git clone -q https://github.com/dscripka/openwakeword 2>/dev/null || echo \"openwakeword already cloned\"\n",
        "!pip install -q -e ./openwakeword --no-deps\n",
        "\n",
        "# Core dependencies\n",
        "!pip install -q mutagen==1.47.0\n",
        "!pip install -q torchinfo==1.8.0\n",
        "!pip install -q torchmetrics==1.2.0\n",
        "!pip install -q speechbrain==0.5.14\n",
        "!pip install -q audiomentations==0.33.0\n",
        "!pip install -q torch-audiomentations==0.11.0\n",
        "!pip install -q acoustics==0.2.6\n",
        "!pip install -q onnxruntime==1.22.1 ai_edge_litert==1.4.0 onnxsim\n",
        "!pip install -q onnx onnx_graphsurgeon sng4onnx\n",
        "!pip install -q onnx_tf tensorflow 2>/dev/null || true  # Prevents train.py crash\n",
        "!pip install -q pronouncing==0.2.0\n",
        "!pip install -q datasets==2.14.6\n",
        "!pip install -q deep-phonemizer==0.0.19\n",
        "\n",
        "print(\"âœ… Dependencies installed\")\n",
        "\n",
        "# ============================================================\n",
        "# NOW import scipy/numpy after installation (fresh import)\n",
        "# ============================================================\n",
        "import numpy as np\n",
        "import scipy.io.wavfile\n",
        "from tqdm.auto import tqdm\n",
        "import datasets\n",
        "\n",
        "print(f\"   numpy version: {np.__version__}\")\n",
        "print(f\"   scipy version: {scipy.__version__}\")\n",
        "\n",
        "# ============================================================\n",
        "# 3.2 DOWNLOAD REQUIRED MODELS\n",
        "# ============================================================\n",
        "print(\"\\nðŸ“¥ Downloading openWakeWord model files...\")\n",
        "\n",
        "model_dir = \"./openwakeword/openwakeword/resources/models\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "model_files = [\n",
        "    (\"embedding_model.onnx\", \"https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.onnx\"),\n",
        "    (\"embedding_model.tflite\", \"https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.tflite\"),\n",
        "    (\"melspectrogram.onnx\", \"https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.onnx\"),\n",
        "    (\"melspectrogram.tflite\", \"https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.tflite\"),\n",
        "]\n",
        "\n",
        "for filename, url in model_files:\n",
        "    filepath = os.path.join(model_dir, filename)\n",
        "    if not os.path.exists(filepath):\n",
        "        !wget -q -O {filepath} {url}\n",
        "        print(f\"   âœ… {filename}\")\n",
        "    else:\n",
        "        print(f\"   â­ï¸ {filename} (already exists)\")\n",
        "\n",
        "# ============================================================\n",
        "# 3.3 DOWNLOAD ROOM IMPULSE RESPONSES\n",
        "# ============================================================\n",
        "print(\"\\nðŸ“¥ Downloading room impulse responses...\")\n",
        "\n",
        "rir_dir = \"./mit_rirs\"\n",
        "if not os.path.exists(rir_dir) or len(os.listdir(rir_dir)) < 100:\n",
        "    os.makedirs(rir_dir, exist_ok=True)\n",
        "\n",
        "    # Install git-lfs\n",
        "    !git lfs install\n",
        "\n",
        "    if not os.path.exists(\"MIT_environmental_impulse_responses\"):\n",
        "        !git clone -q https://huggingface.co/datasets/davidscripka/MIT_environmental_impulse_responses\n",
        "\n",
        "    # Process the RIR files\n",
        "    wav_files = list(Path(\"./MIT_environmental_impulse_responses/16khz\").glob(\"*.wav\"))\n",
        "    if wav_files:\n",
        "        rir_dataset = datasets.Dataset.from_dict({\n",
        "            \"audio\": [str(i) for i in wav_files]\n",
        "        }).cast_column(\"audio\", datasets.Audio())\n",
        "\n",
        "        for row in tqdm(rir_dataset, desc=\"Processing RIRs\"):\n",
        "            name = row['audio']['path'].split('/')[-1]\n",
        "            scipy.io.wavfile.write(\n",
        "                os.path.join(rir_dir, name),\n",
        "                16000,\n",
        "                (row['audio']['array'] * 32767).astype(np.int16)\n",
        "            )\n",
        "        print(f\"   âœ… {len(os.listdir(rir_dir))} RIR files\")\n",
        "    else:\n",
        "        print(\"   âš ï¸ No RIR files found in cloned repo\")\n",
        "else:\n",
        "    print(f\"   â­ï¸ RIRs already downloaded ({len(os.listdir(rir_dir))} files)\")\n",
        "\n",
        "# ============================================================\n",
        "# 3.4 DOWNLOAD BACKGROUND AUDIO\n",
        "# ============================================================\n",
        "print(\"\\nðŸ“¥ Downloading background audio...\")\n",
        "\n",
        "# AudioSet - currently unavailable due to dataset restructuring\n",
        "audioset_dir = \"./audioset_16k\"\n",
        "\n",
        "if not os.path.exists(audioset_dir) or len([f for f in os.listdir(audioset_dir) if f.endswith('.wav')]) < 50:\n",
        "    os.makedirs(audioset_dir, exist_ok=True)\n",
        "\n",
        "    print(\"   â­ï¸ Skipping AudioSet (dataset recently restructured)\")\n",
        "    print(\"   Using FMA + pre-computed features for background audio instead.\")\n",
        "else:\n",
        "    count = len([f for f in os.listdir(audioset_dir) if f.endswith('.wav')])\n",
        "    print(f\"   â­ï¸ AudioSet already downloaded ({count} files)\")\n",
        "\n",
        "# FMA (Free Music Archive)\n",
        "fma_dir = \"./fma\"\n",
        "if not os.path.exists(fma_dir) or len([f for f in os.listdir(fma_dir) if f.endswith('.wav')]) < 50:\n",
        "    os.makedirs(fma_dir, exist_ok=True)\n",
        "    print(\"   Loading FMA dataset (streaming)...\")\n",
        "\n",
        "    try:\n",
        "        fma_dataset = datasets.load_dataset(\"rudraml/fma\", name=\"small\", split=\"train\", streaming=True)\n",
        "        fma_dataset = iter(fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000)))\n",
        "\n",
        "        n_hours = 3  # 3 hours of music clips for better variety\n",
        "        n_clips = n_hours * 3600 // 30  # FMA clips are 30 seconds each\n",
        "\n",
        "        for i in tqdm(range(n_clips), desc=\"Processing FMA\"):\n",
        "            try:\n",
        "                row = next(fma_dataset)\n",
        "                name = row['audio']['path'].split('/')[-1].replace(\".mp3\", \".wav\")\n",
        "                scipy.io.wavfile.write(\n",
        "                    os.path.join(fma_dir, name),\n",
        "                    16000,\n",
        "                    (row['audio']['array'] * 32767).astype(np.int16)\n",
        "                )\n",
        "            except StopIteration:\n",
        "                break\n",
        "            except Exception as e:\n",
        "                continue  # Skip problematic files\n",
        "        print(f\"   âœ… {len([f for f in os.listdir(fma_dir) if f.endswith('.wav')])} FMA files\")\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸ FMA download failed: {e}\")\n",
        "else:\n",
        "    count = len([f for f in os.listdir(fma_dir) if f.endswith('.wav')])\n",
        "    print(f\"   â­ï¸ FMA already downloaded ({count} files)\")\n",
        "\n",
        "# ============================================================\n",
        "# 3.5 DOWNLOAD PRE-COMPUTED FEATURES\n",
        "# ============================================================\n",
        "print(\"\\nðŸ“¥ Downloading pre-computed features (this is the big download)...\")\n",
        "\n",
        "# Training features (~17GB)\n",
        "features_file = \"./openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"\n",
        "if not os.path.exists(features_file):\n",
        "    print(\"   â¬‡ï¸ Downloading training features (~17 GB)...\")\n",
        "    print(\"   This may take 10-30 minutes depending on connection speed.\")\n",
        "    !wget -q --show-progress https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\n",
        "    print(\"   âœ… Training features downloaded\")\n",
        "else:\n",
        "    size_gb = os.path.getsize(features_file) / 1024 / 1024 / 1024\n",
        "    print(f\"   â­ï¸ Training features already downloaded ({size_gb:.1f} GB)\")\n",
        "\n",
        "# Validation features (~176MB)\n",
        "val_file = \"validation_set_features.npy\"\n",
        "if not os.path.exists(val_file):\n",
        "    print(\"   â¬‡ï¸ Downloading validation features (~176 MB)...\")\n",
        "    !wget -q --show-progress https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy\n",
        "    print(\"   âœ… Validation features downloaded\")\n",
        "else:\n",
        "    size_mb = os.path.getsize(val_file) / 1024 / 1024\n",
        "    print(f\"   â­ï¸ Validation features already downloaded ({size_mb:.0f} MB)\")\n",
        "\n",
        "# ============================================================\n",
        "# VERIFICATION\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“Š VERIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def count_wav_files(directory):\n",
        "    if os.path.isdir(directory):\n",
        "        return len([f for f in os.listdir(directory) if f.endswith('.wav')])\n",
        "    return 0\n",
        "\n",
        "rir_count = count_wav_files(rir_dir)\n",
        "audioset_count = count_wav_files(audioset_dir)\n",
        "fma_count = count_wav_files(fma_dir)\n",
        "total_bg = audioset_count + fma_count\n",
        "\n",
        "checks = [\n",
        "    (\"RIRs\", rir_count, 100),\n",
        "    (\"AudioSet\", audioset_count, 0),  # Optional - 0 minimum\n",
        "    (\"FMA\", fma_count, 50),\n",
        "]\n",
        "\n",
        "all_ok = True\n",
        "for name, actual_count, min_count in checks:\n",
        "    if min_count > 0:\n",
        "        status = \"âœ…\" if actual_count >= min_count else \"âš ï¸\"\n",
        "        print(f\"   {status} {name}: {actual_count} files (need {min_count}+)\")\n",
        "        if actual_count < min_count:\n",
        "            all_ok = False\n",
        "    else:\n",
        "        status = \"âœ…\" if actual_count > 0 else \"â­ï¸\"\n",
        "        print(f\"   {status} {name}: {actual_count} files (optional)\")\n",
        "\n",
        "# Check feature files\n",
        "for name, path in [(\"Training features\", features_file), (\"Validation features\", val_file)]:\n",
        "    if os.path.exists(path):\n",
        "        size = os.path.getsize(path)\n",
        "        size_str = f\"{size/1024/1024/1024:.1f} GB\" if size > 1024*1024*1024 else f\"{size/1024/1024:.0f} MB\"\n",
        "        print(f\"   âœ… {name} ({size_str})\")\n",
        "    else:\n",
        "        print(f\"   âŒ {name} (missing)\")\n",
        "        all_ok = False\n",
        "\n",
        "print(f\"\\n   Total background audio: {total_bg} files (need 50+)\")\n",
        "\n",
        "if total_bg < 50:\n",
        "    all_ok = False\n",
        "\n",
        "if all_ok:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"âœ… STEP 3 COMPLETE - All data downloaded!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nðŸ‘‰ Proceed to Step 4 to train your model.\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"âš ï¸ Some downloads may have failed.\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nMinimum requirements:\")\n",
        "    print(\"   â€¢ 50+ total background audio files (AudioSet + FMA)\")\n",
        "    print(\"   â€¢ Training features file (17 GB)\")\n",
        "    print(\"   â€¢ Validation features file (176 MB)\")\n",
        "    print(\"\\nRe-run this cell to retry, or proceed if you have enough background audio.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "step4_train",
        "outputId": "27b5c6d0-d1f1-46d8-a8c3-82e2d8a5231c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ðŸš€ STEP 4: MODEL TRAINING\n",
            "============================================================\n",
            "\n",
            "â˜ï¸  Setting up Google Drive...\n",
            "   Drive already mounted.\n",
            "   ðŸ“‚ Created folder: Google Drive/OpenWakeWord_Models/\n",
            "   âœ… Google Drive connected! Models will be saved there.\n",
            "\n",
            "============================================================\n",
            "ðŸŽ¯ STARTING TRAINING\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "ðŸŽ¯ TRAINING MODEL 1/3: 'how_do_you_wanna_do_this!?'\n",
            "   Model name: how_do_you_wanna_do_this\n",
            "============================================================\n",
            "\n",
            "ðŸ“ Phase 1/3: Generating 30,000 synthetic speech clips...\n",
            "2026-01-21 18:27:02.257939: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769020022.532048   11365 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769020022.607768   11365 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769020023.146432   11365 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769020023.146476   11365 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769020023.146483   11365 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769020023.146487   11365 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 18:27:03.206807: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/pronouncing/__init__.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "/content/piper-sample-generator/generate_samples.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch_model = torch.load(model_path)\n",
            "WARNING:root:Downloading phonemizer model from DeepPhonemizer library...\n",
            "/usr/local/lib/python3.12/dist-packages/dp/model/model.py:306: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "WARNING:root:The word 'how_do_you_wanna_do_this!?' was not found in the pronunciation dictionary! Using the DeepPhonemizer library to predict the phonemes.\n",
            "WARNING:root:Phones for 'how_do_you_wanna_do_this!?': [HH][AW][D][OY][UW][AA][N][AH][D][AH][TH][IH][S]!?\n",
            "WARNING:root:The word 'how_do_you_wanna_do_this!?' was not found in the pronunciation dictionary! Using the DeepPhonemizer library to predict the phonemes.\n",
            "WARNING:root:Phones for 'how_do_you_wanna_do_this!?': [HH][AW][D][OY][UW][AA][N][AH][D][AH][TH][IH][S]!?\n",
            "\n",
            "ðŸ”Š Phase 2/3: Augmenting clips with noise and reverb...\n",
            "2026-01-21 18:52:52.350928: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769021572.371409   18027 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769021572.377663   18027 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769021572.394402   18027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769021572.394434   18027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769021572.394438   18027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769021572.394443   18027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 18:52:52.399417: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/pronouncing/__init__.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "/usr/local/lib/python3.12/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = PitchShift(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = BandStopFilter(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = AddColoredNoise(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = AddBackgroundNoise(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = Gain(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/composition.py:42: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = Compose(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/audiomentations/core/transforms_interface.py:61: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
            "  warnings.warn(\n",
            "Computing features: 100% 1874/1875 [21:47<00:00,  1.43it/s]\n",
            "Trimming empty rows: 30it [00:00, 43.94it/s]            \n",
            "Computing features: 100% 1874/1875 [21:13<00:00,  1.47it/s]\n",
            "Trimming empty rows: 30it [00:00, 34.44it/s]            \n",
            "Computing features: 100% 187/187 [02:09<00:00,  1.45it/s]\n",
            "Trimming empty rows: 3it [00:00, 40.57it/s]   \n",
            "Computing features: 100% 187/187 [02:09<00:00,  1.44it/s]\n",
            "Trimming empty rows: 3it [00:00, 41.61it/s]   \n",
            "\n",
            "ðŸ§  Phase 3/3: Training neural network (30,000 steps)...\n",
            "2026-01-21 19:40:39.256088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769024439.276356   63216 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769024439.282612   63216 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769024439.299074   63216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769024439.299106   63216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769024439.299110   63216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769024439.299116   63216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 19:40:39.304301: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/pronouncing/__init__.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "Training: 100% 29999/30000 [13:27<00:00, 37.17it/s]\n",
            "Training: 100% 2999/3000.0 [03:39<00:00, 13.64it/s]\n",
            "Training: 100% 2999/3000.0 [03:50<00:00, 13.02it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/openwakeword/openwakeword/train.py\", line 909, in <module>\n",
            "    convert_onnx_to_tflite(os.path.join(config[\"output_dir\"], config[\"model_name\"] + \".onnx\"),\n",
            "  File \"/content/openwakeword/openwakeword/train.py\", line 578, in convert_onnx_to_tflite\n",
            "    from onnx_tf.backend import prepare\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnx_tf/__init__.py\", line 1, in <module>\n",
            "    from . import backend\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnx_tf/backend.py\", line 24, in <module>\n",
            "    from onnx_tf.common import data_type\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnx_tf/common/data_type.py\", line 4, in <module>\n",
            "    from onnx import mapping\n",
            "ImportError: cannot import name 'mapping' from 'onnx' (/usr/local/lib/python3.12/dist-packages/onnx/__init__.py). Did you mean: '_mapping'?\n",
            "\n",
            "âœ… SUCCESS: ./my_custom_model/how_do_you_wanna_do_this.onnx (618.4 KB)\n",
            "\n",
            "â˜ï¸  SAVED TO GOOGLE DRIVE: how_do_you_wanna_do_this.onnx (618.4 KB)\n",
            "   Location: Google Drive/OpenWakeWord_Models/how_do_you_wanna_do_this.onnx\n",
            "\n",
            "============================================================\n",
            "ðŸŽ¯ TRAINING MODEL 2/3: 'how_you_wanna_do_this!?'\n",
            "   Model name: how_you_wanna_do_this\n",
            "============================================================\n",
            "\n",
            "ðŸ“ Phase 1/3: Generating 30,000 synthetic speech clips...\n",
            "2026-01-21 20:02:21.664035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769025741.859419   68752 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769025741.911481   68752 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769025742.341666   68752 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769025742.341717   68752 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769025742.341722   68752 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769025742.341726   68752 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 20:02:22.388916: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/pronouncing/__init__.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "/content/piper-sample-generator/generate_samples.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch_model = torch.load(model_path)\n",
            "/usr/local/lib/python3.12/dist-packages/dp/model/model.py:306: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "WARNING:root:The word 'how_you_wanna_do_this!?' was not found in the pronunciation dictionary! Using the DeepPhonemizer library to predict the phonemes.\n",
            "WARNING:root:Phones for 'how_you_wanna_do_this!?': [HH][AW][Y][UW][W][AE][N][AA][D][AA][TH][IH][S]!?\n",
            "WARNING:root:The word 'how_you_wanna_do_this!?' was not found in the pronunciation dictionary! Using the DeepPhonemizer library to predict the phonemes.\n",
            "WARNING:root:Phones for 'how_you_wanna_do_this!?': [HH][AW][Y][UW][W][AE][N][AA][D][AA][TH][IH][S]!?\n",
            "\n",
            "ðŸ”Š Phase 2/3: Augmenting clips with noise and reverb...\n",
            "2026-01-21 20:29:15.211023: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769027355.231696   75612 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769027355.237937   75612 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769027355.253856   75612 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769027355.253898   75612 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769027355.253906   75612 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769027355.253910   75612 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 20:29:15.258519: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/pronouncing/__init__.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "/usr/local/lib/python3.12/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = PitchShift(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = BandStopFilter(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = AddColoredNoise(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = AddBackgroundNoise(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = Gain(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/composition.py:42: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = Compose(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/audiomentations/core/transforms_interface.py:61: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
            "  warnings.warn(\n",
            "Computing features: 100% 1874/1875 [21:55<00:00,  1.42it/s]\n",
            "Trimming empty rows: 30it [00:00, 44.01it/s]            \n",
            "Computing features: 100% 1874/1875 [21:30<00:00,  1.45it/s]\n",
            "Trimming empty rows: 30it [00:00, 43.47it/s]            \n",
            "Computing features: 100% 187/187 [02:11<00:00,  1.43it/s]\n",
            "Trimming empty rows: 3it [00:00, 31.20it/s]   \n",
            "Computing features: 100% 187/187 [02:07<00:00,  1.46it/s]\n",
            "Trimming empty rows: 3it [00:00, 39.74it/s]   \n",
            "\n",
            "ðŸ§  Phase 3/3: Training neural network (30,000 steps)...\n",
            "2026-01-21 21:17:30.211586: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769030250.244301  120866 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769030250.254181  120866 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769030250.279156  120866 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769030250.279196  120866 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769030250.279201  120866 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769030250.279208  120866 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 21:17:30.286423: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/pronouncing/__init__.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "Training: 100% 29999/30000 [13:14<00:00, 37.75it/s]\n",
            "Training: 100% 2999/3000.0 [03:41<00:00, 13.54it/s]\n",
            "Training: 100% 2999/3000.0 [03:44<00:00, 13.33it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/openwakeword/openwakeword/train.py\", line 909, in <module>\n",
            "    convert_onnx_to_tflite(os.path.join(config[\"output_dir\"], config[\"model_name\"] + \".onnx\"),\n",
            "  File \"/content/openwakeword/openwakeword/train.py\", line 578, in convert_onnx_to_tflite\n",
            "    from onnx_tf.backend import prepare\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnx_tf/__init__.py\", line 1, in <module>\n",
            "    from . import backend\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnx_tf/backend.py\", line 24, in <module>\n",
            "    from onnx_tf.common import data_type\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnx_tf/common/data_type.py\", line 4, in <module>\n",
            "    from onnx import mapping\n",
            "ImportError: cannot import name 'mapping' from 'onnx' (/usr/local/lib/python3.12/dist-packages/onnx/__init__.py). Did you mean: '_mapping'?\n",
            "\n",
            "âœ… SUCCESS: ./my_custom_model/how_you_wanna_do_this.onnx (618.4 KB)\n",
            "\n",
            "â˜ï¸  SAVED TO GOOGLE DRIVE: how_you_wanna_do_this.onnx (618.4 KB)\n",
            "   Location: Google Drive/OpenWakeWord_Models/how_you_wanna_do_this.onnx\n",
            "\n",
            "============================================================\n",
            "ðŸŽ¯ TRAINING MODEL 3/3: 'how_you_do_this!?'\n",
            "   Model name: how_you_do_this\n",
            "============================================================\n",
            "\n",
            "ðŸ“ Phase 1/3: Generating 30,000 synthetic speech clips...\n",
            "2026-01-21 21:38:55.184633: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769031535.365647  126333 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769031535.421188  126333 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769031535.848363  126333 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769031535.848419  126333 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769031535.848427  126333 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769031535.848434  126333 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 21:38:55.895053: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/pronouncing/__init__.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "/content/piper-sample-generator/generate_samples.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch_model = torch.load(model_path)\n",
            "/usr/local/lib/python3.12/dist-packages/dp/model/model.py:306: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "WARNING:root:The word 'how_you_do_this!?' was not found in the pronunciation dictionary! Using the DeepPhonemizer library to predict the phonemes.\n",
            "WARNING:root:Phones for 'how_you_do_this!?': [HH][AW][Y][UW][D][OW][TH][IH][S]!?\n",
            "WARNING:root:The word 'how_you_do_this!?' was not found in the pronunciation dictionary! Using the DeepPhonemizer library to predict the phonemes.\n",
            "WARNING:root:Phones for 'how_you_do_this!?': [HH][AW][Y][UW][D][OW][TH][IH][S]!?\n",
            "\n",
            "ðŸ”Š Phase 2/3: Augmenting clips with noise and reverb...\n",
            "2026-01-21 21:59:33.368410: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769032773.390730  131631 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769032773.396984  131631 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769032773.413482  131631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769032773.413508  131631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769032773.413511  131631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769032773.413515  131631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 21:59:33.418260: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/pronouncing/__init__.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "/usr/local/lib/python3.12/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = PitchShift(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = BandStopFilter(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = AddColoredNoise(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = AddBackgroundNoise(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = Gain(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/composition.py:42: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = Compose(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/audiomentations/core/transforms_interface.py:61: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
            "  warnings.warn(\n",
            "Computing features: 100% 1874/1875 [21:18<00:00,  1.47it/s]\n",
            "Trimming empty rows: 30it [00:00, 43.62it/s]            \n",
            "Computing features: 100% 1874/1875 [21:08<00:00,  1.48it/s]\n",
            "Trimming empty rows: 30it [00:00, 41.63it/s]            \n",
            "Computing features: 100% 187/187 [02:06<00:00,  1.48it/s]\n",
            "Trimming empty rows: 3it [00:00, 37.32it/s]   \n",
            "Computing features: 100% 187/187 [02:07<00:00,  1.47it/s]\n",
            "Trimming empty rows: 3it [00:00, 39.56it/s]   \n",
            "\n",
            "ðŸ§  Phase 3/3: Training neural network (30,000 steps)...\n",
            "2026-01-21 22:46:41.627683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769035601.647935  176671 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769035601.654182  176671 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769035601.671271  176671 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769035601.671295  176671 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769035601.671300  176671 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769035601.671304  176671 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-21 22:46:41.675934: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/pronouncing/__init__.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "Training: 100% 29999/30000 [13:15<00:00, 37.70it/s]\n",
            "Training: 100% 2999/3000.0 [03:38<00:00, 13.75it/s]\n",
            "Training: 100% 2999/3000.0 [03:41<00:00, 13.55it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/openwakeword/openwakeword/train.py\", line 909, in <module>\n",
            "    convert_onnx_to_tflite(os.path.join(config[\"output_dir\"], config[\"model_name\"] + \".onnx\"),\n",
            "  File \"/content/openwakeword/openwakeword/train.py\", line 578, in convert_onnx_to_tflite\n",
            "    from onnx_tf.backend import prepare\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnx_tf/__init__.py\", line 1, in <module>\n",
            "    from . import backend\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnx_tf/backend.py\", line 24, in <module>\n",
            "    from onnx_tf.common import data_type\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnx_tf/common/data_type.py\", line 4, in <module>\n",
            "    from onnx import mapping\n",
            "ImportError: cannot import name 'mapping' from 'onnx' (/usr/local/lib/python3.12/dist-packages/onnx/__init__.py). Did you mean: '_mapping'?\n",
            "\n",
            "âœ… SUCCESS: ./my_custom_model/how_you_do_this.onnx (618.4 KB)\n",
            "\n",
            "â˜ï¸  SAVED TO GOOGLE DRIVE: how_you_do_this.onnx (618.4 KB)\n",
            "   Location: Google Drive/OpenWakeWord_Models/how_you_do_this.onnx\n",
            "\n",
            "============================================================\n",
            "ðŸ“Š TRAINING SUMMARY\n",
            "============================================================\n",
            "\n",
            "âœ… Successful: 3\n",
            "   â€¢ how_do_you_wanna_do_this!? â†’ ./my_custom_model/how_do_you_wanna_do_this.onnx\n",
            "   â€¢ how_you_wanna_do_this!? â†’ ./my_custom_model/how_you_wanna_do_this.onnx\n",
            "   â€¢ how_you_do_this!? â†’ ./my_custom_model/how_you_do_this.onnx\n",
            "\n",
            "â˜ï¸  SAVED TO GOOGLE DRIVE: 3 model(s)\n",
            "   Location: Google Drive/OpenWakeWord_Models/\n",
            "   â€¢ how_do_you_wanna_do_this.onnx\n",
            "   â€¢ how_you_wanna_do_this.onnx\n",
            "   â€¢ how_you_do_this.onnx\n",
            "\n",
            "âœ¨ Your models are safely stored in Google Drive!\n",
            "   You can access them anytime, even after this session ends.\n"
          ]
        }
      ],
      "source": [
        "# @title ## ðŸš€ Step 4: Train Models { display-mode: \"form\" }\n",
        "# @markdown This trains a model for each wake word you specified.\n",
        "# @markdown\n",
        "# @markdown **Time:** ~30-90 minutes per model (depends on settings and hardware)\n",
        "# @markdown\n",
        "# @markdown **Training phases:**\n",
        "# @markdown 1. Generate synthetic speech clips\n",
        "# @markdown 2. Augment clips with noise/reverb\n",
        "# @markdown 3. Train neural network\n",
        "# @markdown 4. Export to ONNX format\n",
        "# @markdown\n",
        "# @markdown ---\n",
        "# @markdown ### ðŸ“ Google Drive Settings (Recommended!)\n",
        "# @markdown Colab's browser download can be unreliable. Google Drive ensures your models are saved safely.\n",
        "\n",
        "enable_google_drive = True # @param {type:\"boolean\"}\n",
        "# @markdown â†‘ Enable to save models directly to Google Drive as soon as they finish.\n",
        "\n",
        "drive_folder_name = \"OpenWakeWord_Models\" # @param {type:\"string\"}\n",
        "# @markdown â†‘ Folder name in your Google Drive (created automatically if it doesn't exist).\n",
        "\n",
        "import yaml\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸš€ STEP 4: MODEL TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================================\n",
        "# GOOGLE DRIVE SETUP (if enabled)\n",
        "# ============================================================\n",
        "gdrive_enabled = False\n",
        "gdrive_path = None\n",
        "\n",
        "if enable_google_drive:\n",
        "    print(\"\\nâ˜ï¸  Setting up Google Drive...\")\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "\n",
        "        # Check if already mounted\n",
        "        if not os.path.ismount('/content/drive'):\n",
        "            print(\"   (You may be prompted to authorize access)\\n\")\n",
        "            drive.mount('/content/drive')\n",
        "        else:\n",
        "            print(\"   Drive already mounted.\")\n",
        "\n",
        "        # Create the output folder\n",
        "        drive_base = '/content/drive/MyDrive'\n",
        "        drive_output_path = os.path.join(drive_base, drive_folder_name)\n",
        "\n",
        "        if not os.path.exists(drive_output_path):\n",
        "            os.makedirs(drive_output_path)\n",
        "            print(f\"   ðŸ“‚ Created folder: Google Drive/{drive_folder_name}/\")\n",
        "        else:\n",
        "            print(f\"   ðŸ“‚ Using folder: Google Drive/{drive_folder_name}/\")\n",
        "\n",
        "        # Verify write access\n",
        "        test_file = os.path.join(drive_output_path, '.test_write')\n",
        "        with open(test_file, 'w') as f:\n",
        "            f.write('test')\n",
        "        os.remove(test_file)\n",
        "\n",
        "        gdrive_enabled = True\n",
        "        gdrive_path = drive_output_path\n",
        "        print(f\"   âœ… Google Drive connected! Models will be saved there.\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"   âš ï¸ Google Drive only available in Colab. Using local storage.\")\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸ Drive setup failed: {e}\")\n",
        "        print(\"   Models will be downloaded via browser instead.\")\n",
        "else:\n",
        "    print(\"\\nðŸ’¾ Google Drive: DISABLED\")\n",
        "    print(\"   Models will be downloaded via browser after training.\")\n",
        "    print(\"   âš ï¸ Note: Browser downloads can be unreliable in Colab.\")\n",
        "\n",
        "def sanitize_name(name):\n",
        "    \"\"\"Convert wake word to valid filename.\"\"\"\n",
        "    return re.sub(r'[^a-zA-Z0-9]+', '_', name).strip('_')\n",
        "\n",
        "def save_to_drive(onnx_path, model_name):\n",
        "    \"\"\"Copy model to Google Drive. Returns True if successful.\"\"\"\n",
        "    if not gdrive_enabled or not gdrive_path:\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        dest_path = os.path.join(gdrive_path, f\"{model_name}.onnx\")\n",
        "        shutil.copy2(onnx_path, dest_path)\n",
        "\n",
        "        # Verify the copy\n",
        "        if os.path.exists(dest_path):\n",
        "            size_kb = os.path.getsize(dest_path) / 1024\n",
        "            print(f\"\\nâ˜ï¸  SAVED TO GOOGLE DRIVE: {model_name}.onnx ({size_kb:.1f} KB)\")\n",
        "            print(f\"   Location: Google Drive/{drive_folder_name}/{model_name}.onnx\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"\\nâš ï¸  Drive copy verification failed for {model_name}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâš ï¸  Failed to save to Drive: {e}\")\n",
        "        return False\n",
        "\n",
        "def queue_download(onnx_path, model_name):\n",
        "    \"\"\"Queue a model file for browser download (Colab only). Non-blocking.\"\"\"\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        import threading\n",
        "\n",
        "        def trigger_download():\n",
        "            try:\n",
        "                files.download(onnx_path)\n",
        "            except:\n",
        "                pass  # Ignore errors in background thread\n",
        "\n",
        "        print(f\"\\nâ¬‡ï¸  Queued {model_name}.onnx for download\")\n",
        "        thread = threading.Thread(target=trigger_download)\n",
        "        thread.daemon = True\n",
        "        thread.start()\n",
        "\n",
        "        import time\n",
        "        time.sleep(1)\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(f\"\\nðŸ“ Not running in Colab - find your model at: {onnx_path}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâš ï¸  Auto-download skipped: {e}\")\n",
        "        return False\n",
        "\n",
        "# ============================================================\n",
        "# LOAD CONFIG AND START TRAINING\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸŽ¯ STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "base_config = yaml.load(\n",
        "    open(\"openwakeword/examples/custom_model.yml\", 'r').read(),\n",
        "    yaml.Loader\n",
        ")\n",
        "\n",
        "output_dir = \"./my_custom_model\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "successful_models = []\n",
        "failed_models = []\n",
        "models_saved_to_drive = []\n",
        "models_pending_download = []\n",
        "\n",
        "for i, word in enumerate(wake_word_list):\n",
        "    model_name = sanitize_name(word)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ðŸŽ¯ TRAINING MODEL {i+1}/{len(wake_word_list)}: '{word}'\")\n",
        "    print(f\"   Model name: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Create config for this word\n",
        "    config = base_config.copy()\n",
        "    config[\"target_phrase\"] = [word]\n",
        "    config[\"model_name\"] = model_name\n",
        "    config[\"n_samples\"] = number_of_examples\n",
        "    config[\"n_samples_val\"] = max(500, number_of_examples // 10)\n",
        "    config[\"steps\"] = number_of_training_steps\n",
        "    config[\"target_accuracy\"] = target_accuracy\n",
        "    config[\"target_recall\"] = target_recall\n",
        "    config[\"target_false_positives_per_hour\"] = target_false_positives_per_hour\n",
        "    config[\"output_dir\"] = output_dir\n",
        "    config[\"max_negative_weight\"] = false_activation_penalty\n",
        "    config[\"layer_size\"] = layer_size\n",
        "    config[\"background_paths\"] = ['./audioset_16k', './fma']\n",
        "    config[\"false_positive_validation_data_path\"] = \"validation_set_features.npy\"\n",
        "    config[\"feature_data_files\"] = {\"ACAV100M_sample\": \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\"}\n",
        "\n",
        "    config_file = f'{model_name}_config.yaml'\n",
        "    with open(config_file, 'w') as f:\n",
        "        yaml.dump(config, f)\n",
        "\n",
        "    try:\n",
        "        # Phase 1: Generate clips\n",
        "        print(f\"\\nðŸ“ Phase 1/3: Generating {number_of_examples:,} synthetic speech clips...\")\n",
        "        !{sys.executable} openwakeword/openwakeword/train.py --training_config {config_file} --generate_clips\n",
        "\n",
        "        # Phase 2: Augment clips\n",
        "        print(f\"\\nðŸ”Š Phase 2/3: Augmenting clips with noise and reverb...\")\n",
        "        !{sys.executable} openwakeword/openwakeword/train.py --training_config {config_file} --augment_clips\n",
        "\n",
        "        # Phase 3: Train model\n",
        "        print(f\"\\nðŸ§  Phase 3/3: Training neural network ({number_of_training_steps:,} steps)...\")\n",
        "        !{sys.executable} openwakeword/openwakeword/train.py --training_config {config_file} --train_model\n",
        "\n",
        "        # Check if ONNX was created\n",
        "        onnx_path = f\"{output_dir}/{model_name}.onnx\"\n",
        "        if os.path.exists(onnx_path):\n",
        "            size_kb = os.path.getsize(onnx_path) / 1024\n",
        "            print(f\"\\nâœ… SUCCESS: {onnx_path} ({size_kb:.1f} KB)\")\n",
        "\n",
        "            model_info = {\n",
        "                'word': word,\n",
        "                'model_name': model_name,\n",
        "                'onnx_path': onnx_path\n",
        "            }\n",
        "            successful_models.append(model_info)\n",
        "\n",
        "            # Try to save to Google Drive first\n",
        "            if gdrive_enabled and save_to_drive(onnx_path, model_name):\n",
        "                models_saved_to_drive.append(model_info)\n",
        "            else:\n",
        "                # Fall back to queuing download (if not using Drive)\n",
        "                models_pending_download.append(model_info)\n",
        "        else:\n",
        "            print(f\"\\nâŒ ONNX model not found at {onnx_path}\")\n",
        "            failed_models.append({'word': word, 'error': 'ONNX not created'})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Training failed: {e}\")\n",
        "        failed_models.append({'word': word, 'error': str(e)})\n",
        "\n",
        "# ============================================================\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"ðŸ“Š TRAINING SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nâœ… Successful: {len(successful_models)}\")\n",
        "for m in successful_models:\n",
        "    print(f\"   â€¢ {m['word']} â†’ {m['onnx_path']}\")\n",
        "\n",
        "if failed_models:\n",
        "    print(f\"\\nâŒ Failed: {len(failed_models)}\")\n",
        "    for m in failed_models:\n",
        "        print(f\"   â€¢ {m['word']}: {m['error']}\")\n",
        "\n",
        "# Report on saving method\n",
        "if models_saved_to_drive:\n",
        "    print(f\"\\nâ˜ï¸  SAVED TO GOOGLE DRIVE: {len(models_saved_to_drive)} model(s)\")\n",
        "    print(f\"   Location: Google Drive/{drive_folder_name}/\")\n",
        "    for m in models_saved_to_drive:\n",
        "        print(f\"   â€¢ {m['model_name']}.onnx\")\n",
        "    print(f\"\\nâœ¨ Your models are safely stored in Google Drive!\")\n",
        "    print(f\"   You can access them anytime, even after this session ends.\")\n",
        "\n",
        "if models_pending_download:\n",
        "    print(f\"\\nâ¬‡ï¸  PENDING DOWNLOAD: {len(models_pending_download)} model(s)\")\n",
        "    print(f\"   Run Step 5 to download these models.\")\n",
        "\n",
        "if not successful_models:\n",
        "    print(f\"\\nâš ï¸ No models were trained successfully. Check the errors above.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step5_download",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "ad1c98f2-b277-4c0e-f5d4-d716129dfb31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "â¬‡ï¸ STEP 5: DOWNLOAD MODELS\n",
            "============================================================\n",
            "\n",
            "â˜ï¸  Google Drive Status: CONNECTED\n",
            "   Your models are already saved to:\n",
            "   Google Drive/OpenWakeWord_Models/\n",
            "\n",
            "   The download below is a backup copy.\n",
            "\n",
            "ðŸ“¦ Generated Models:\n",
            "\n",
            "Model                               Size            Drive Status\n",
            "-----------------------------------------------------------------\n",
            "how_do_you_wanna_do_this.onnx       618.4 KB      âœ… Saved\n",
            "how_you_wanna_do_this.onnx          618.4 KB      âœ… Saved\n",
            "how_you_do_this.onnx                618.4 KB      âœ… Saved\n",
            "\n",
            "ðŸ“ Created archive: openwakeword_models_20260121_230746.zip\n",
            "   Size: 1708.4 KB\n",
            "\n",
            "â¬‡ï¸ Starting download...\n",
            "   (If download doesn't start, check your browser's download folder)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_15e67bd9-b760-4a41-8f60-0da2f297a7e8\", \"openwakeword_models_20260121_230746.zip\", 1749394)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“¥ Individual file downloads:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_387314ce-fe21-4164-9917-a2cbf23ea396\", \"how_do_you_wanna_do_this.onnx\", 633208)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… how_do_you_wanna_do_this.onnx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9e2438fc-b06d-49ab-944b-fe25502498e6\", \"how_you_wanna_do_this.onnx\", 633208)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… how_you_wanna_do_this.onnx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7ffbac33-2da5-4d2c-a375-40cfa71512fe\", \"how_you_do_this.onnx\", 633208)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… how_you_do_this.onnx\n",
            "\n",
            "ðŸ’¡ Remember: Your models are also in Google Drive!\n",
            "   Google Drive/OpenWakeWord_Models/\n"
          ]
        }
      ],
      "source": [
        "# @title ## â¬‡ï¸ Step 5: Download Your Models { display-mode: \"form\" }\n",
        "# @markdown Downloads all generated model files via browser.\n",
        "# @markdown\n",
        "# @markdown **Note:** If you enabled Google Drive in Step 4, your models are already saved there!\n",
        "# @markdown This step is mainly for backup or if you disabled Google Drive.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"â¬‡ï¸ STEP 5: DOWNLOAD MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check for variables from Step 4\n",
        "try:\n",
        "    _gdrive_enabled = gdrive_enabled\n",
        "    _gdrive_path = gdrive_path\n",
        "    _drive_folder = drive_folder_name\n",
        "except NameError:\n",
        "    _gdrive_enabled = False\n",
        "    _gdrive_path = None\n",
        "    _drive_folder = \"OpenWakeWord_Models\"\n",
        "\n",
        "try:\n",
        "    models_to_download = successful_models\n",
        "except NameError:\n",
        "    models_to_download = []\n",
        "\n",
        "if not models_to_download:\n",
        "    print(\"\\nâš ï¸ No models to download. Run Step 4 first.\")\n",
        "else:\n",
        "    # Show Google Drive status\n",
        "    if _gdrive_enabled and _gdrive_path:\n",
        "        print(f\"\\nâ˜ï¸  Google Drive Status: CONNECTED\")\n",
        "        print(f\"   Your models are already saved to:\")\n",
        "        print(f\"   Google Drive/{_drive_folder}/\")\n",
        "        print(f\"\\n   The download below is a backup copy.\")\n",
        "\n",
        "    print(f\"\\nðŸ“¦ Generated Models:\\n\")\n",
        "    print(f\"{'Model':<35} {'Size':<15} {'Drive Status'}\")\n",
        "    print(f\"{'-'*65}\")\n",
        "\n",
        "    download_files = []\n",
        "    output_dir = \"./my_custom_model\"\n",
        "\n",
        "    for m in models_to_download:\n",
        "        model_name = m['model_name']\n",
        "        onnx_path = m.get('onnx_path', f\"{output_dir}/{model_name}.onnx\")\n",
        "\n",
        "        # Check local file\n",
        "        if os.path.exists(onnx_path):\n",
        "            size_kb = os.path.getsize(onnx_path) / 1024\n",
        "\n",
        "            # Check if it's in Drive\n",
        "            drive_status = \"â€”\"\n",
        "            if _gdrive_enabled and _gdrive_path:\n",
        "                drive_file = os.path.join(_gdrive_path, f\"{model_name}.onnx\")\n",
        "                if os.path.exists(drive_file):\n",
        "                    drive_status = \"âœ… Saved\"\n",
        "                else:\n",
        "                    drive_status = \"âŒ Missing\"\n",
        "\n",
        "            print(f\"{model_name}.onnx{' '*(30-len(model_name))} {size_kb:.1f} KB{' '*(10-len(f'{size_kb:.1f}'))} {drive_status}\")\n",
        "            download_files.append(onnx_path)\n",
        "        else:\n",
        "            print(f\"{model_name}.onnx{' '*(30-len(model_name))} âŒ not found\")\n",
        "\n",
        "    # Offer to save missing files to Drive\n",
        "    if _gdrive_enabled and _gdrive_path:\n",
        "        missing_from_drive = []\n",
        "        for m in models_to_download:\n",
        "            model_name = m['model_name']\n",
        "            onnx_path = m.get('onnx_path', f\"{output_dir}/{model_name}.onnx\")\n",
        "            drive_file = os.path.join(_gdrive_path, f\"{model_name}.onnx\")\n",
        "            if os.path.exists(onnx_path) and not os.path.exists(drive_file):\n",
        "                missing_from_drive.append((onnx_path, model_name))\n",
        "\n",
        "        if missing_from_drive:\n",
        "            print(f\"\\nðŸ“¤ Copying {len(missing_from_drive)} missing model(s) to Google Drive...\")\n",
        "            for onnx_path, model_name in missing_from_drive:\n",
        "                try:\n",
        "                    dest_path = os.path.join(_gdrive_path, f\"{model_name}.onnx\")\n",
        "                    shutil.copy2(onnx_path, dest_path)\n",
        "                    print(f\"   âœ… {model_name}.onnx\")\n",
        "                except Exception as e:\n",
        "                    print(f\"   âŒ {model_name}.onnx - {e}\")\n",
        "\n",
        "    # Create zip archive and trigger download\n",
        "    if download_files:\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        zip_name = f'openwakeword_models_{timestamp}'\n",
        "\n",
        "        # Copy files to temp directory for zipping\n",
        "        os.makedirs(zip_name, exist_ok=True)\n",
        "        for f in download_files:\n",
        "            shutil.copy(f, zip_name)\n",
        "\n",
        "        shutil.make_archive(zip_name, 'zip', zip_name)\n",
        "        zip_path = f'{zip_name}.zip'\n",
        "\n",
        "        print(f\"\\nðŸ“ Created archive: {zip_path}\")\n",
        "        print(f\"   Size: {os.path.getsize(zip_path) / 1024:.1f} KB\")\n",
        "\n",
        "        # Auto-download in Colab\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            print(\"\\nâ¬‡ï¸ Starting download...\")\n",
        "            print(\"   (If download doesn't start, check your browser's download folder)\")\n",
        "\n",
        "            # Download zip first\n",
        "            files.download(zip_path)\n",
        "\n",
        "            # Also offer individual files\n",
        "            print(\"\\nðŸ“¥ Individual file downloads:\")\n",
        "            for f in download_files:\n",
        "                try:\n",
        "                    files.download(f)\n",
        "                    print(f\"   âœ… {os.path.basename(f)}\")\n",
        "                except:\n",
        "                    print(f\"   âš ï¸ {os.path.basename(f)} - download may have failed\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(f\"\\nðŸ“¥ Download manually from the file browser on the left.\")\n",
        "\n",
        "        # Cleanup temp dir\n",
        "        shutil.rmtree(zip_name, ignore_errors=True)\n",
        "\n",
        "        if _gdrive_enabled:\n",
        "            print(f\"\\nðŸ’¡ Remember: Your models are also in Google Drive!\")\n",
        "            print(f\"   Google Drive/{_drive_folder}/\")\n",
        "    else:\n",
        "        print(\"\\nâš ï¸ No model files found to download.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step6_test",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14b71221-e38d-4987-d42d-18d6918ab1d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ðŸ§ª STEP 6: MODEL TESTING\n",
            "============================================================\n",
            "\n",
            "âš ï¸ Could not import openwakeword: No module named 'openwakeword.model'\n",
            "   Models were still created - you can test them in your own environment.\n"
          ]
        }
      ],
      "source": [
        "# @title ## ðŸ§ª Step 6 (Optional): Test Your Models { display-mode: \"form\" }\n",
        "# @markdown Quick sanity check that your models load and run.\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸ§ª STEP 6: MODEL TESTING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    models_to_test = successful_models\n",
        "except NameError:\n",
        "    models_to_test = []\n",
        "\n",
        "output_dir = \"./my_custom_model\"\n",
        "\n",
        "if not models_to_test:\n",
        "    print(\"\\nâš ï¸ No models to test. Run Step 4 first.\")\n",
        "else:\n",
        "    try:\n",
        "        import openwakeword\n",
        "        from openwakeword.model import Model\n",
        "\n",
        "        for m in models_to_test:\n",
        "            model_name = m['model_name']\n",
        "            word = m['word']\n",
        "            onnx_path = m.get('onnx_path', f\"{output_dir}/{model_name}.onnx\")\n",
        "\n",
        "            print(f\"\\nðŸ“Š Testing: {word} ({model_name})\")\n",
        "\n",
        "            if os.path.exists(onnx_path):\n",
        "                try:\n",
        "                    model = Model(\n",
        "                        wakeword_models=[onnx_path],\n",
        "                        inference_framework='onnx'\n",
        "                    )\n",
        "\n",
        "                    # Test with silence (should not trigger)\n",
        "                    test_audio = np.zeros(16000, dtype=np.int16)\n",
        "                    prediction = model.predict(test_audio)\n",
        "\n",
        "                    print(f\"   âœ… Model loaded successfully\")\n",
        "                    print(f\"   Prediction on silence: {prediction}\")\n",
        "                    print(f\"   (Should be close to 0.0 - no wake word in silence)\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"   âŒ Error testing model: {e}\")\n",
        "            else:\n",
        "                print(f\"   âš ï¸ Model file not found: {onnx_path}\")\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"\\nâš ï¸ Could not import openwakeword: {e}\")\n",
        "        print(\"   Models were still created - you can test them in your own environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usage_docs"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ“– How to Use Your Models\n",
        "\n",
        "### Home Assistant\n",
        "\n",
        "1. Download your `.onnx` model file (from Google Drive or Step 5)\n",
        "2. Copy to your Home Assistant config: `/config/openwakeword/`\n",
        "3. In the openWakeWord add-on settings, add your custom model path\n",
        "4. Restart the add-on\n",
        "\n",
        "See: [Home Assistant openWakeWord docs](https://github.com/home-assistant/addons/blob/master/openwakeword/DOCS.md#custom-wake-word-models)\n",
        "\n",
        "### Python\n",
        "\n",
        "```python\n",
        "from openwakeword.model import Model\n",
        "import pyaudio\n",
        "import numpy as np\n",
        "\n",
        "# Load your model\n",
        "model = Model(wakeword_models=['path/to/your_model.onnx'])\n",
        "\n",
        "# Setup audio stream\n",
        "pa = pyaudio.PyAudio()\n",
        "stream = pa.open(\n",
        "    rate=16000,\n",
        "    channels=1,\n",
        "    format=pyaudio.paInt16,\n",
        "    input=True,\n",
        "    frames_per_buffer=1280\n",
        ")\n",
        "\n",
        "# Listen for wake word\n",
        "print(\"Listening for wake word...\")\n",
        "while True:\n",
        "    audio = np.frombuffer(stream.read(1280), dtype=np.int16)\n",
        "    prediction = model.predict(audio)\n",
        "    \n",
        "    for model_name, score in prediction.items():\n",
        "        if score > 0.5:  # Adjust threshold as needed\n",
        "            print(f\"Wake word detected! Score: {score:.3f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”§ Troubleshooting\n",
        "\n",
        "### Model doesn't detect well\n",
        "- Try different phonetic spellings in Step 1\n",
        "- Increase `number_of_examples` to 40,000+\n",
        "- Increase `number_of_training_steps` to 40,000+\n",
        "- Lower the detection threshold (e.g., 0.3 instead of 0.5)\n",
        "\n",
        "### Too many false activations\n",
        "- Increase `false_activation_penalty` (try 2000-3000)\n",
        "- Lower `target_false_positives_per_hour` (try 0.1)\n",
        "- Raise the detection threshold (e.g., 0.7 instead of 0.5)\n",
        "\n",
        "### Training times out\n",
        "- Reduce `number_of_examples` to 10,000-20,000\n",
        "- Reduce `number_of_training_steps` to 10,000-20,000\n",
        "- Use Google Colab Pro for longer runtimes\n",
        "- Use GPU runtime (faster training)\n",
        "\n",
        "### Download hangs or fails\n",
        "- **Enable Google Drive in Step 4!** This provides reliable model saving\n",
        "- Models are copied to Drive immediately after each one completes\n",
        "- Even if Colab disconnects, your models are safe in Drive\n",
        "- Re-run Step 5 to retry browser downloads if needed\n",
        "\n",
        "### \"ModuleNotFoundError\" during training\n",
        "- This is usually harmless - check if the `.onnx` file was still created\n",
        "- The training script may show errors about optional components\n",
        "\n",
        "### \"RecursionError\" in Step 1\n",
        "- This was a bug in the torch.load patching - now fixed!\n",
        "- If you still see it, restart the runtime and try again\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“š Resources\n",
        "\n",
        "- [openWakeWord GitHub](https://github.com/dscripka/openWakeWord)\n",
        "- [openWakeWord Documentation](https://github.com/dscripka/openWakeWord/blob/main/docs/)\n",
        "- [Home Assistant Voice](https://www.home-assistant.io/voice_control/)\n",
        "- [Piper TTS](https://github.com/rhasspy/piper) (used for synthetic speech)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "421df195e35f4850aff87e9d2555c460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8463a8f7fb554b1bbb6d3ba308a84469",
              "IPY_MODEL_809a9fe89abc4be3a387c5ee6819344a",
              "IPY_MODEL_1b133123024b413281257d46b81c9e8e"
            ],
            "layout": "IPY_MODEL_08e3acb099424fbfa3da78fc1eebf7d7"
          }
        },
        "8463a8f7fb554b1bbb6d3ba308a84469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dd8c36806f844a39d6369193d865054",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_efecf1a6b1874dab9501f64f40644e14",
            "value": "Processingâ€‡RIRs:â€‡100%"
          }
        },
        "809a9fe89abc4be3a387c5ee6819344a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26cb651928d44a559bd15ef48c2ac44a",
            "max": 270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05cc5e6929e14caf8858ea24bb6bcea9",
            "value": 270
          }
        },
        "1b133123024b413281257d46b81c9e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8171c39e38fa415994d6616144c6d1ea",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d688eb8606414c64b62749021e4e7697",
            "value": "â€‡270/270â€‡[00:13&lt;00:00,â€‡15.50it/s]"
          }
        },
        "08e3acb099424fbfa3da78fc1eebf7d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dd8c36806f844a39d6369193d865054": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efecf1a6b1874dab9501f64f40644e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26cb651928d44a559bd15ef48c2ac44a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05cc5e6929e14caf8858ea24bb6bcea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8171c39e38fa415994d6616144c6d1ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d688eb8606414c64b62749021e4e7697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3f1a7bc816c4727a1f993a5f9e5b57f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de82196453cc400fbe90163bd873336e",
              "IPY_MODEL_dd3411d25355420bab2eed01f65f8d1a",
              "IPY_MODEL_f75c499b1d724e019d068098cf2112ff"
            ],
            "layout": "IPY_MODEL_bb325949c78c4b54abd0c925882b7574"
          }
        },
        "de82196453cc400fbe90163bd873336e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_766b28b2aca64ab3a99dc06d25cea186",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_294ccd510c8543398f508691e6954138",
            "value": "Downloadingâ€‡builderâ€‡script:â€‡"
          }
        },
        "dd3411d25355420bab2eed01f65f8d1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81b5e50a85c84000a96727efdca61839",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbed3a5442b94df286b3fc787f2889cd",
            "value": 1
          }
        },
        "f75c499b1d724e019d068098cf2112ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c686b096009f4b6081fb084bd7c6429a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e32d89205eae47f19bfd3ddc6e19de78",
            "value": "â€‡46.3k/?â€‡[00:00&lt;00:00,â€‡4.29MB/s]"
          }
        },
        "bb325949c78c4b54abd0c925882b7574": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "766b28b2aca64ab3a99dc06d25cea186": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "294ccd510c8543398f508691e6954138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81b5e50a85c84000a96727efdca61839": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cbed3a5442b94df286b3fc787f2889cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c686b096009f4b6081fb084bd7c6429a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e32d89205eae47f19bfd3ddc6e19de78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30081236282042dcac967cf84ce8948f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d1f83fecac24d9ab3f872e3a7385aea",
              "IPY_MODEL_30ac513416ae487bb403913e665a86e5",
              "IPY_MODEL_e74a0beb5d9d4420bc5b5ad9702677cb"
            ],
            "layout": "IPY_MODEL_62177c5656c1439dbd804caedeed8b49"
          }
        },
        "6d1f83fecac24d9ab3f872e3a7385aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59dd67ecc6c74472a0a128efecf22ea3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4cc9d5f0a4374713bc09cfe82d481fef",
            "value": "Downloadingâ€‡readme:â€‡100%"
          }
        },
        "30ac513416ae487bb403913e665a86e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25cd4e75474d43aab56da846a77841f7",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c3451b379e34d55ad0c6afd24637eb4",
            "value": 25
          }
        },
        "e74a0beb5d9d4420bc5b5ad9702677cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7648886f26724a949c4c892109df96cc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2256dc63f4aa4813bccb9aba4f80d569",
            "value": "â€‡25.0/25.0â€‡[00:00&lt;00:00,â€‡3.12kB/s]"
          }
        },
        "62177c5656c1439dbd804caedeed8b49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59dd67ecc6c74472a0a128efecf22ea3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cc9d5f0a4374713bc09cfe82d481fef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25cd4e75474d43aab56da846a77841f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c3451b379e34d55ad0c6afd24637eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7648886f26724a949c4c892109df96cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2256dc63f4aa4813bccb9aba4f80d569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f5d1c2e59904b5fa1f7533838a423ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb604e5737c74ebd83d8b15783e0e5bf",
              "IPY_MODEL_ff6a41d55d5741cfa7c164b254a7c8f3",
              "IPY_MODEL_60595598068b483594d9b38d9e610c13"
            ],
            "layout": "IPY_MODEL_4e32e4b8b99c492f975685849e5e920a"
          }
        },
        "cb604e5737c74ebd83d8b15783e0e5bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d908bf28df44b768d2f97d5f5478dae",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f01ff8764c46462a9f1f61aa7cf9d5c5",
            "value": "Processingâ€‡FMA:â€‡100%"
          }
        },
        "ff6a41d55d5741cfa7c164b254a7c8f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a14dcee8015c495491d39093e6625a59",
            "max": 360,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63c5fd23ed504dd38f12a8bde9b5db35",
            "value": 360
          }
        },
        "60595598068b483594d9b38d9e610c13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d667412a92d41a0baedc878090053ef",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5800542945ee4d6380a8bc7808c607e0",
            "value": "â€‡360/360â€‡[01:53&lt;00:00,â€‡â€‡3.16it/s]"
          }
        },
        "4e32e4b8b99c492f975685849e5e920a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d908bf28df44b768d2f97d5f5478dae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f01ff8764c46462a9f1f61aa7cf9d5c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a14dcee8015c495491d39093e6625a59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63c5fd23ed504dd38f12a8bde9b5db35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d667412a92d41a0baedc878090053ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5800542945ee4d6380a8bc7808c607e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
